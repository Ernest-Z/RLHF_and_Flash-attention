{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a31f87-c0a8-4a31-8777-edece096488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "本部分基于微软DeepSpeed模型进行代码编写\n",
    "'''\n",
    "#提示工程数据集引入\n",
    "class DahoasRmstaticDataset(PromptRawDataset):\n",
    "    #定义相关参数\n",
    "    def __init__(self, ooutput_path, seed, local_rank, dataset_name):\n",
    "        super().__init__(output_path, seed, local_rank, dataset_name)\n",
    "        self.dataset_name = \"Dahoas/rm-static\"\n",
    "        self.dataset_name_clean = \"Dahoas_rm_static\"\n",
    "    #获取训练数据\n",
    "    def get_train_date(self):\n",
    "        return self.raw_datasets[\"train\"]\n",
    "    #获取评估数据\n",
    "    def get_eval_data(self):\n",
    "        return raw_datasets['test']\n",
    "    #获取提示\n",
    "    def get_prompt(self, sample):\n",
    "        return sample['prompt']\n",
    "    #获取积极回复\n",
    "    def get_chosen(self, sample):\n",
    "        return sample['chosen']\n",
    "    #获取负面回复\n",
    "    def get_rejected(self, sample):\n",
    "        return sample['rejected']\n",
    "    #与提示相互绑定\n",
    "    def get_prompt_and_chosen(self, sample):\n",
    "        return sample['prompt'] + sample['chosen']\n",
    "    def get_prompt_and_rejected(self, sample):\n",
    "        return sample['prompt'] + sample['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e52dc-c22e-4d17-99fb-5379be72ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "引入训练模型的数据：\n",
    "训练阶段（train_phase）\n",
    "1.监督微调大模型：使用sample['prompt'] + sample['chosen']\n",
    "2.训练奖励模型：sample['prompt'] + sample['chosen'] & sample['prompt'] + sample['rejected']\n",
    "3.训练强化学习模型：sample['prompt']\n",
    "'''\n",
    "if train_phase == 1:\n",
    "    for i, tmp_data in enumerate(current_dataset):\n",
    "        # 标记积极回复文本\n",
    "        chosen_sentence = raw_dataset.get_prompt_and_chosen(tmp_data)\n",
    "        if chosen_sentence is not None:\n",
    "            chosen_sentence += end_of_conversation_token\n",
    "            chosen_token = tokenizer(chosen_sentence,\n",
    "                                     max_length=max_seq_len,\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True,\n",
    "                                     return_tensors=\"pt\")\n",
    "            chosen_token[\"input_ids\"] = chosen_token[\"input_ids\"].squeeze(0)\n",
    "            chosen_token[\"attention_mask\"] = chosen_token[\"attention_mask\"].squeeze(0)\n",
    "            chosen_dataset.append(chosen_token)\n",
    " \n",
    "elif train_phase == 2:\n",
    "    for i, tmp_data in enumerate(current_dataset):\n",
    "        # 标记积极和负面回复文本\n",
    "        chosen_sentence = raw_dataset.get_prompt_and_chosen(tmp_data)\n",
    "        reject_sentence = raw_dataset.get_prompt_and_rejected(tmp_data)  \n",
    "        if chosen_sentence is not None and reject_sentence is not None:\n",
    "            chosen_sentence += end_of_conversation_token \n",
    "            reject_sentence += end_of_conversation_token\n",
    "            chosen_token = tokenizer(chosen_sentence,\n",
    "                                     max_length=max_seq_len,\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True,\n",
    "                                     return_tensors=\"pt\")\n",
    "            reject_token = tokenizer(reject_sentence,\n",
    "                                     max_length=max_seq_len,\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True,\n",
    "                                     return_tensors=\"pt\")\n",
    "            chosen_token[\"input_ids\"] = chosen_token[\"input_ids\"]\n",
    "            chosen_token[\"attention_mask\"] = chosen_token[\"attention_mask\"]\n",
    "            chosen_dataset.append(chosen_token)\n",
    " \n",
    "            reject_token[\"input_ids\"] = reject_token[\"input_ids\"]\n",
    "            reject_token[\"attention_mask\"] = reject_token[\"attention_mask\"]\n",
    "            reject_dataset.append(reject_token)\n",
    " \n",
    "elif train_phase == 3:\n",
    "    for i, tmp_data in enumerate(current_dataset):\n",
    "    # 仅标记提示\n",
    "        prompt = raw_dataset.get_prompt(tmp_data)\n",
    "        if prompt is not None:\n",
    "            prompt_token = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            prompt_token[\"input_ids\"] = prompt_token[\"input_ids\"]\n",
    "            prompt_token[\"attention_mask\"] = prompt_token[\"attention_mask\"]\n",
    "            for key_word in [\"input_ids\", \"attention_mask\"]:\n",
    "                length = prompt_token[key_word].size()[-1]\n",
    "                if length > max_seq_len:\n",
    "                    y = prompt_token[key_word].squeeze(0)[length -(max_seq_len - 1):].flip(0)\n",
    "                else:\n",
    "                    y = prompt_token[key_word].squeeze(0).flip(0)\n",
    "                prompt_token[key_word] = y\n",
    "            prompt_dataset.append(prompt_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c7f46-adcf-4c9f-a81d-6e34b3bfde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.监督微调大模型：使用方法--ZeRO优化方法，其应用部分可看：https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7677d2-cc05-41e5-bcad-f0ea080c80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.训练奖励模型\n",
    "'''\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_model,\n",
    "                 tokenizer,\n",
    "                 num_padding_at_beginning=0,\n",
    "                 compute_fp32_loss=False):\n",
    "        super().__init__()\n",
    "        self.config = base_model.config\n",
    "        self.num_padding_at_beginning = num_padding_at_beginning\n",
    "        if hasattr(self.config, \"word_embed_proj_dim\"):\n",
    "            self.v_head = nn.Linear(self.config.word_embed_proj_dim, 1, bias=False)\n",
    "        else:\n",
    "            # `n_embd`可用`hidden_size`替代\n",
    "            self.config.n_embd = self.config.hidden_size if hasattr(\n",
    "                self.config, \"hidden_size\") else self.config.n_embd\n",
    "            self.v_head = nn.Linear(self.config.n_embd, 1, bias=False)\n",
    "        self.rwtransformer = base_model\n",
    "        self.PAD_ID = tokenizer.pad_token_id\n",
    "        self.compute_fp32_loss = compute_fp32_loss\n",
    "\n",
    "    def gradient_checkpointing_enable(self):\n",
    "        self.rwtransformer.gradient_checkpointing_enable()\n",
    "\n",
    "    def gradient_checkpointing_disable(self):\n",
    "        self.rwtransformer.gradient_checkpointing_disable()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                past_key_values=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                use_cache=False):\n",
    "        loss = None\n",
    "\n",
    "        if self.config.model_type == \"llama\":\n",
    "            kwargs = dict()\n",
    "        else:\n",
    "            kwargs = dict(head_mask=head_mask)\n",
    "\n",
    "        transformer_outputs = self.rwtransformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            **kwargs)\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.v_head(hidden_states).squeeze(-1)\n",
    "        chosen_mean_scores = []\n",
    "        rejected_mean_scores = []\n",
    "\n",
    "        # 将输入和回报划分为两个部分：积极和负面\n",
    "        assert len(input_ids.shape) == 2\n",
    "        bs = input_ids.shape[0] // 2\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        chosen_ids = input_ids[:bs]  # bs x seq x 1\n",
    "        rejected_ids = input_ids[bs:]\n",
    "        chosen_rewards = rewards[:bs]\n",
    "        rejected_rewards = rewards[bs:]\n",
    "\n",
    "        # 计算成对损失。在填充之前仅对不同标记进行反向传播\n",
    "        loss = 0.\n",
    "        for i in range(bs):\n",
    "            chosen_id = chosen_ids[i]\n",
    "            rejected_id = rejected_ids[i]\n",
    "            chosen_reward = chosen_rewards[i]\n",
    "            rejected_reward = rejected_rewards[i]\n",
    "\n",
    "            c_inds = (chosen_id == self.PAD_ID).nonzero()\n",
    "            c_ind = c_inds[self.num_padding_at_beginning].item() if len(\n",
    "                c_inds\n",
    "            ) > self.num_padding_at_beginning else seq_len  \n",
    "            check_divergence = (chosen_id != rejected_id).nonzero()\n",
    "\n",
    "            if len(check_divergence) == 0:\n",
    "                end_ind = rejected_reward.size(-1)\n",
    "                divergence_ind = end_ind - 1\n",
    "                r_ind = c_ind\n",
    "            else:\n",
    "                # 判断是否需要填充，否则取语句长度值\n",
    "                r_inds = (rejected_id == self.PAD_ID).nonzero()\n",
    "                r_ind = r_inds[self.num_padding_at_beginning].item(\n",
    "                ) if len(r_inds) > self.num_padding_at_beginning else seq_len\n",
    "                end_ind = max(c_ind, r_ind)\n",
    "                divergence_ind = check_divergence[0]\n",
    "            assert divergence_ind > 0\n",
    "            c_truncated_reward = chosen_reward[divergence_ind:end_ind]\n",
    "            r_truncated_reward = rejected_reward[divergence_ind:end_ind]\n",
    "            chosen_mean_scores.append(\n",
    "                chosen_reward[c_ind - 1])  #使用引用末端得分\n",
    "            rejected_mean_scores.append(rejected_reward[r_ind - 1])\n",
    "\n",
    "            if self.compute_fp32_loss:\n",
    "                c_truncated_reward = c_truncated_reward.float()\n",
    "                r_truncated_reward = r_truncated_reward.float()\n",
    "            loss += -torch.nn.functional.logsigmoid(c_truncated_reward -\n",
    "                                                    r_truncated_reward).mean()\n",
    "\n",
    "        loss = loss / bs\n",
    "        chosen_mean_scores = torch.stack(chosen_mean_scores)\n",
    "        rejected_mean_scores = torch.stack(rejected_mean_scores)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"chosen_mean_scores\": chosen_mean_scores,\n",
    "            \"rejected_mean_scores\": rejected_mean_scores,\n",
    "        }\n",
    "\n",
    "    def forward_value(self,\n",
    "                      input_ids=None,\n",
    "                      attention_mask=None,\n",
    "                      past_key_values=None,\n",
    "                      position_ids=None,\n",
    "                      head_mask=None,\n",
    "                      inputs_embeds=None,\n",
    "                      return_value_only=False,\n",
    "                      prompt_length=0,\n",
    "                      use_cache=False):\n",
    "\n",
    "        if self.config.model_type == \"llama\":\n",
    "            kwargs = dict()\n",
    "        else:\n",
    "            kwargs = dict(head_mask=head_mask)\n",
    "\n",
    "        transformer_outputs = self.rwtransformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            **kwargs)\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        values = self.v_head(hidden_states).squeeze(-1)\n",
    "        if return_value_only:\n",
    "            return values\n",
    "        else:\n",
    "            # [prompt, answer, 0, 0, 0, 0] 正规矩阵\n",
    "            assert prompt_length > 1, \"prompt_length must be greater than 1 to help select the end score\"\n",
    "            bs = values.size(0)\n",
    "            seq_len = input_ids.shape[1]\n",
    "            chosen_end_scores = [\n",
    "            ]  # 使用方程与常见的前向传递方程一致\n",
    "            for i in range(bs):\n",
    "                input_id = input_ids[i]\n",
    "                value = values[i]\n",
    "\n",
    "                c_inds = (input_id[prompt_length:] == self.PAD_ID).nonzero()\n",
    "                # 只使用序列的答案部分，可忽略开头的填充\n",
    "                c_ind = c_inds[0].item() + prompt_length if len(\n",
    "                    c_inds) > 0 else seq_len\n",
    "                chosen_end_scores.append(value[c_ind - 1])\n",
    "            return {\n",
    "                \"values\": values,\n",
    "                \"chosen_end_scores\": torch.stack(chosen_end_scores),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fc18e-1fae-40f1-980b-53e1a1d96a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.基于人类反馈的强化学习模型（RLHF）,源代码如下\n",
    "'''\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import deepspeed\n",
    "from deepspeed.ops.adam import FusedAdam\n",
    "from deepspeed.ops.adam import DeepSpeedCPUAdam\n",
    "from transformers import AutoModelForCausalLM, get_scheduler\n",
    "\n",
    "from dschat.utils.ds_utils import get_train_ds_config, get_eval_ds_config\n",
    "from dschat.utils.module.lora import convert_linear_layer_to_lora, only_optimize_lora_parameters, make_model_gradient_checkpointing_compatible\n",
    "from dschat.utils.model.model_utils import create_hf_model, create_critic_model\n",
    "from dschat.utils.utils import get_optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def log_init(model_name, stime=None):\n",
    "    if torch.distributed.get_rank() == 0:\n",
    "        tag = \"start\" if stime is None else \"end\"\n",
    "        suffix = \"ing\" if stime is None else \"ed\"\n",
    "        duration = \"\"\n",
    "        if stime is not None:\n",
    "            duration = \"(duration: {:.2f}s)\".format(time.time() - stime)\n",
    "        msg = f\"[{tag}] Initializ{suffix} {model_name} Model [{tag}] {duration}\"\n",
    "        stars = (90 - len(msg)) // 2\n",
    "        extra_star = \"*\" if (90 - len(msg)) % 2 == 1 else \"\"\n",
    "        print(\"*\" * stars + msg + \"*\" * stars + extra_star)\n",
    "        return time.time()\n",
    "\n",
    "\n",
    "class DeepSpeedRLHFEngine():\n",
    "\n",
    "    def __init__(self, actor_model_name_or_path, critic_model_name_or_path,\n",
    "                 tokenizer, args, num_total_iters):\n",
    "        self.args = args\n",
    "        self.num_total_iters = num_total_iters\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.actor = self._init_actor(\n",
    "            actor_model_name_or_path=actor_model_name_or_path)\n",
    "        self.ref = self._init_ref(\n",
    "            actor_model_name_or_path=actor_model_name_or_path)\n",
    "        self.actor_ema = None\n",
    "        if self.args.enable_ema:\n",
    "            self.actor_ema = self._init_ema(\n",
    "                actor_model_name_or_path=actor_model_name_or_path)\n",
    "        self.critic = self._init_critic(\n",
    "            critic_model_name_or_path=critic_model_name_or_path)\n",
    "        self.reward = self._init_reward(\n",
    "            critic_model_name_or_path=critic_model_name_or_path)\n",
    "        if self.args.critic_gradient_checkpointing:\n",
    "            self.critic.gradient_checkpointing_enable()\n",
    "\n",
    "    def _init_actor(self, actor_model_name_or_path):\n",
    "        stime = log_init(\"Actor\")\n",
    "\n",
    "        # 需要使用dschat库的相关函数，以下是需要的参数\n",
    "        ds_config = get_train_ds_config(\n",
    "            offload=self.args.offload,    #是否需要卸载\n",
    "            dtype=self.args.dtype,\n",
    "            stage=self.args.actor_zero_stage,  #动作0状态\n",
    "            enable_hybrid_engine=self.args.enable_hybrid_engine,  #是否多机协作\n",
    "            inference_tp_size=self.args.inference_tp_size,   #推理器算力范围\n",
    "            release_inference_cache=self.args.release_inference_cache,  #释放推理器缓存\n",
    "            pin_parameters=(not self.args.unpin_actor_parameters),\n",
    "            tp_gather_partition_size=self.args.tp_gather_partition_size,\n",
    "            max_out_tokens=self.args.max_prompt_seq_len +\n",
    "            self.args.max_answer_seq_len,\n",
    "            enable_tensorboard=self.args.enable_tensorboard,\n",
    "            enable_mixed_precision_lora=self.args.enable_mixed_precision_lora,\n",
    "            tb_path=self.args.tensorboard_path,\n",
    "            tb_name=\"step3_actor\")\n",
    "        ds_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        #这里优化是以梯度上升进行优化\n",
    "        ds_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps_actor\n",
    "\n",
    "        # 模型\n",
    "        actor_model = create_hf_model(\n",
    "            model_class=AutoModelForCausalLM,\n",
    "            model_name_or_path=actor_model_name_or_path,\n",
    "            tokenizer=self.tokenizer,\n",
    "            ds_config=ds_config,\n",
    "            dropout=self.args.actor_dropout)\n",
    "\n",
    "        # 将线性层进行LoRA扩频，作为动作模型\n",
    "        if self.args.actor_lora_dim > 0:\n",
    "            actor_model = convert_linear_layer_to_lora(\n",
    "                actor_model, self.args.actor_lora_module_name,\n",
    "                self.args.actor_lora_dim)\n",
    "            if self.args.only_optimize_lora:\n",
    "                actor_model = only_optimize_lora_parameters(actor_model)\n",
    "                actor_model = make_model_gradient_checkpointing_compatible(\n",
    "                    actor_model)\n",
    "\n",
    "        # 优化器\n",
    "        AdamOptimizer = DeepSpeedCPUAdam if self.args.offload else FusedAdam\n",
    "        optim_params = get_optimizer_grouped_parameters(\n",
    "            actor_model, self.args.actor_weight_decay,\n",
    "            self.args.actor_lora_learning_rate)\n",
    "        optim = AdamOptimizer(optim_params,\n",
    "                              lr=self.args.actor_learning_rate,\n",
    "                              betas=(0.9, 0.95))\n",
    "\n",
    "        # 学习率策略：梯度上升\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=self.args.lr_scheduler_type,\n",
    "            optimizer=optim,\n",
    "            num_warmup_steps=self.args.num_warmup_steps,\n",
    "            num_training_steps=self.num_total_iters,\n",
    "        )\n",
    "\n",
    "        # 加载DeepSpeed算法引擎（单G or 多G）\n",
    "        actor_engine, *_ = deepspeed.initialize(model=actor_model,\n",
    "                                                optimizer=optim,\n",
    "                                                lr_scheduler=lr_scheduler,\n",
    "                                                config=ds_config)\n",
    "\n",
    "        log_init(\"Actor\", stime=stime)\n",
    "\n",
    "        return actor_engine\n",
    "\n",
    "    def _init_ref(self, actor_model_name_or_path):\n",
    "        stime = log_init(\"Ref\")\n",
    "        zero_stage = self.args.actor_zero_stage\n",
    "        if zero_stage != 3:\n",
    "            # 如果 actor 是 ZeRO-3 ，那么将它用于所有事情，否则假设有足够的内存用于参考模型\n",
    "            zero_stage = 0\n",
    "        ds_config = get_eval_ds_config(self.args.offload_reference_model,\n",
    "                                       self.args.dtype, zero_stage)\n",
    "        ds_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        #这里优化是以梯度上升进行优化\n",
    "        ds_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps_actor\n",
    "\n",
    "        ref_model = create_hf_model(AutoModelForCausalLM,\n",
    "                                    actor_model_name_or_path, self.tokenizer,\n",
    "                                    ds_config)\n",
    "\n",
    "        ref_engine, *_ = deepspeed.initialize(model=ref_model,\n",
    "                                              config=ds_config)\n",
    "\n",
    "        log_init(\"Ref\", stime=stime)\n",
    "        return ref_engine\n",
    "\n",
    "    def _init_ema(self, actor_model_name_or_path):\n",
    "        stime = log_init(\"EMA\")\n",
    "        zero_stage = self.args.actor_zero_stage\n",
    "        if zero_stage != 3:\n",
    "            # 如果 actor 是 ZeRO-3 ，那么将它用于所有事情，否则假设有足够的内存用于参考模型\n",
    "            zero_stage = 0\n",
    "        ds_config = get_eval_ds_config(self.args.offload_reference_model,\n",
    "                                       self.args.dtype, zero_stage)\n",
    "        ds_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        #这里优化是以梯度上升进行优化\n",
    "        ds_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps_actor\n",
    "\n",
    "        actor_model_ema = create_hf_model(AutoModelForCausalLM,\n",
    "                                          actor_model_name_or_path,\n",
    "                                          self.tokenizer, ds_config)\n",
    "        if self.args.actor_lora_dim > 0:\n",
    "            actor_model_ema = convert_linear_layer_to_lora(\n",
    "                actor_model_ema, self.args.actor_lora_module_name,\n",
    "                self.args.actor_lora_dim)\n",
    "\n",
    "        ema_engine, *_ = deepspeed.initialize(model=actor_model_ema,\n",
    "                                              config=ds_config)\n",
    "\n",
    "        log_init(\"EMA\", stime=stime)\n",
    "        return ema_engine\n",
    "\n",
    "    def _init_critic(self, critic_model_name_or_path):\n",
    "        stime = log_init(\"Critic\")\n",
    "        ds_config = get_train_ds_config(\n",
    "            offload=self.args.offload,\n",
    "            dtype=self.args.dtype,\n",
    "            stage=self.args.critic_zero_stage,\n",
    "            enable_tensorboard=self.args.enable_tensorboard,\n",
    "            tb_path=self.args.tensorboard_path,\n",
    "            tb_name=\"step3_critic\")\n",
    "        ds_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        #这里优化是以梯度上升进行优化\n",
    "        ds_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps\n",
    "\n",
    "        ds_eval_config = get_eval_ds_config(offload=False,\n",
    "                                            dtype=self.args.dtype,\n",
    "                                            stage=self.args.critic_zero_stage)\n",
    "        # 需要在这里设置训练批量大小和微批量大小以通过 DeepSpeed 引擎的健全性检查。\n",
    "        ds_eval_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        ds_eval_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps\n",
    "\n",
    "        # 判断模型\n",
    "        critic_model = create_critic_model(\n",
    "            model_name_or_path=critic_model_name_or_path,\n",
    "            tokenizer=self.tokenizer,\n",
    "            ds_config=ds_eval_config,\n",
    "            num_padding_at_beginning=self.args.num_padding_at_beginning,\n",
    "            rlhf_training=True,\n",
    "            dropout=self.args.critic_dropout,\n",
    "            zero_stage=self.args.critic_zero_stage)\n",
    "\n",
    "        # 将线性层进行LoRA扩频判断模型\n",
    "        if self.args.critic_lora_dim > 0:\n",
    "            critic_model = convert_linear_layer_to_lora(\n",
    "                critic_model, self.args.critic_lora_module_name,\n",
    "                self.args.critic_lora_dim)\n",
    "            if self.args.only_optimize_lora:\n",
    "                critic_model = only_optimize_lora_parameters(critic_model)\n",
    "                critic_model = make_model_gradient_checkpointing_compatible(\n",
    "                    critic_model)\n",
    "\n",
    "        # 优化器\n",
    "        AdamOptimizer = DeepSpeedCPUAdam if self.args.offload else FusedAdam\n",
    "        optim_params = get_optimizer_grouped_parameters(\n",
    "            critic_model, self.args.critic_weight_decay,\n",
    "            self.args.critic_lora_learning_rate)\n",
    "        optim = AdamOptimizer(optim_params,\n",
    "                              lr=self.args.critic_learning_rate,\n",
    "                              betas=(0.9, 0.95))\n",
    "\n",
    "        # 学习率策略：梯度上升\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=self.args.lr_scheduler_type,\n",
    "            optimizer=optim,\n",
    "            num_warmup_steps=self.args.num_warmup_steps,\n",
    "            num_training_steps=self.num_total_iters,\n",
    "        )\n",
    "\n",
    "        # DeepSpeed Engine\n",
    "        critic_engine, *_ = deepspeed.initialize(model=critic_model,\n",
    "                                                 optimizer=optim,\n",
    "                                                 lr_scheduler=lr_scheduler,\n",
    "                                                 config=ds_config)\n",
    "\n",
    "        log_init(\"Critic\", stime=stime)\n",
    "        return critic_engine\n",
    "\n",
    "    def _init_reward(self, critic_model_name_or_path):\n",
    "        stime = log_init(\"Reward\")\n",
    "        zero_stage = self.args.critic_zero_stage\n",
    "        if zero_stage != 3:\n",
    "            zero_stage = 0\n",
    "\n",
    "        ds_config = get_eval_ds_config(offload=self.args.offload,\n",
    "                                       dtype=self.args.dtype,\n",
    "                                       stage=zero_stage)\n",
    "        ds_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        ds_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps\n",
    "\n",
    "        ds_eval_config = get_eval_ds_config(offload=False,\n",
    "                                            dtype=self.args.dtype,\n",
    "                                            stage=zero_stage)\n",
    "\n",
    "\n",
    "        ds_eval_config[\n",
    "            'train_micro_batch_size_per_gpu'] = self.args.per_device_training_batch_size\n",
    "        ds_eval_config[\n",
    "            'train_batch_size'] = self.args.per_device_training_batch_size * torch.distributed.get_world_size(\n",
    "            ) * self.args.gradient_accumulation_steps\n",
    "\n",
    "        # 奖励模型\n",
    "        reward_model = create_critic_model(\n",
    "            model_name_or_path=critic_model_name_or_path,\n",
    "            tokenizer=self.tokenizer,\n",
    "            ds_config=ds_eval_config,\n",
    "            num_padding_at_beginning=self.args.num_padding_at_beginning,\n",
    "            rlhf_training=True,\n",
    "            dropout=self.args.critic_dropout,\n",
    "            zero_stage=zero_stage)\n",
    "\n",
    "        reward_engine, *_ = deepspeed.initialize(model=reward_model,\n",
    "                                                 config=ds_config)\n",
    "\n",
    "        log_init(\"Reward\", stime=stime)\n",
    "        return reward_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9a843-ab1d-4b60-86ab-9666c02ced50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "近端策略优化PPO：见https://blog.csdn.net/chaishen10000/article/details/131232948?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522171471717316800178561933%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=171471717316800178561933&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-131232948-null-null.142^v100^pc_search_result_base3&utm_term=RLHF&spm=1018.2226.3001.4187\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
